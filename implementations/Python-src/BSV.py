from __future__ import annotations

import io
from copy import deepcopy
from itertools import zip_longest

from columns import *
from columns import _make_header_str

"""
BSV Module: can both read and write!

This specific file contains classes and methods for file I/O and for
the creation of table types.  Implementations of column types are found
in columns.py

More information on the BSV format itself is (currently) found in the
top-level README of this git repository.

The README in this Python-src folder is on implementation quirks and details
on this Python implementation.
"""

LINE_BREAKS: str = FILE + GROUP

NAMES_DICT: "Dict[chr, str]" = {
    FILE: "File",
    GROUP: "Group",
    RECORD: "Record",
    UNIT: "Unit",
}


class RawLine(NamedTuple):
    """The raw line data for a BSV file.

    Ending meanings:

    - GROUP SEPARATOR = end of line/record/row
    - FILE SEPARATOR = end of table, client should read next table
    - '' = end of physical file

    other values may cause undefined behavior
    """

    content: List[str]
    ending: chr
    row_index: int = -1
    starts_after: int = -1


def _split_file_into_rows(
    f, buffer: int = io.DEFAULT_BUFFER_SIZE, **_options
) -> "Generator[RawLine]":
    """
    Reads a file and splits into "lines" terminated with either \x1C or \x1D.
    Essentially a very fancy str.splitlines(True)

    This implementation incrementally loads the file from disk into memory so that
    large files may be loaded without excessive RAM requirements.

    :param f: an opened File object in read mode
    :param buffer: read this many characters at a time
    :return: a RawLine
    """
    # setup
    start_after: int = 0  # for debugging purposes
    done: bool = False
    row_index = 1

    # r: raw buffer in RAM off the disk
    # o: output list (split by RECORD SEPARATOR)
    # s: string buffer to add to output
    o, r, s = [], "", ""  # initialize empty buffers

    # read the file until it's all done
    while not done:
        start_after = f.tell()
        r: str = f.read(buffer)
        if not r:  # nothing more in the file, process the current s buffer and exit
            done = True
        s += r  # prepend any residual output buffer

        # take advantage of the fact that splitlines() includes the file and group separators
        # otherwise, we'd need regular expressions
        lines = s.splitlines(True)  # keep the terminal character for proper processing

        s = ""  # the output buffer has been split for analysis; time to clear it
        for line in lines:  # is there something more efficient here?
            if not s and line == "\n":
                continue  # strip convenience lines
            # print(repr(line))  # debugging
            end = line[-1]
            s += line[:-1]
            if end in LINE_BREAKS:
                yield RawLine(o + [s], end, row_index, start_after)
                o, s = [], ""  # reset the output string
                row_index += 1  # increment for the next row
                continue
            if end == RECORD:
                o.append(s)
                s = ""
                continue
            s += end  # it's not one of the line breaks we care about just yet

    """
    yield out the final line
    it's s[-1] to strip out the trailing newline
    r == "" at this point.
    """
    yield RawLine(o + [s[:-1]], r, row_index, start_after)


class TableRow(abc.ABC):
    """Base class for all rows read from a BSV file."""

    columns = {}
    allow_short = True
    allow_extras = True
    num_columns = 0  # hush the linter
    original_name: str = ""  # see above

    def __init__(self, *fields, extras: List[str] = None, row_index: int = -1):
        """
        This empty method is replaced with the __init__ generated by make_dataclass.
        It is listed here for the benefit of IDEs, linters, and library users to understand
        the __init__ method of a concrete subclass of TableRow.
        :param fields: The number of required fields is defined during the creation of the
            TableRow subclass via new_table_type.  Passing an incorrect number of fields to
            the constructor will raise an error.  The proper number of fields can be determined
            via class attribute num_columns.
        :param extras: An optional list of extra fields.  During processing, it is converted to a
            list of lists of strings (some or all of which may be empty)
        :param row_index: row index in the input for debugging purposes; optional.
        """
        pass

    def __post_init__(self, *fields):
        self.errors = ErrorList()
        *fields, raw_extras, self.row_index = fields
        self.extras = [x.split(UNIT) for x in raw_extras]
        for col_num, (value, (c_name, c_type)) in enumerate(
            zip(fields, self.columns.items())
        ):
            if value is None and not self.allow_short and not self.errors:
                self.errors.append(
                    RowTooShortError(
                        "Line too short",
                        fields,
                        f"Only {col_num} column(s).",
                        self.row_index,
                        severity=ErrorSeverity.ROW_MALFORMATION,
                    )
                )
            setattr(self, c_name, c_type(value, self.errors, self.row_index))
        if self.extras and not self.allow_extras:
            self.errors.append(
                RowTooLongError(
                    "Too many fields!",
                    fields,
                    f"{len(self.extras)} extra fields were provided for"
                    + f" a table with {len(self.columns)}.",
                    self.row_index,
                    severity=ErrorSeverity.ROW_MALFORMATION,
                )
            )

    @classmethod
    def table_str(cls, use_n: bool = True) -> str:
        """The opposite of new_table_from_raw_lines

        :return: a string that will generate this RowType
        """
        # pprint([c.definition_string for c in cls.columns.values()])  # debugging
        return (
            _make_header_str(
                [
                    getattr(cls, "original_name", cls.__name__),  # name
                    ("X" if cls.allow_extras else "")
                    + ("S" if cls.allow_short else ""),  # options
                    getattr(cls, "comment", None),  # comment
                    getattr(cls, "client", None),  # client
                ]
                + getattr(cls, "extra_headers", []),
                RECORD,
            )
            + GROUP
            + ("\n" if use_n else "")
            + RECORD.join([c.definition_string for c in cls.columns.values()])
        )

    def __init_subclass__(cls, **kwargs):
        """These two class attributes are best set here rather than defined as properties"""
        super().__init_subclass__(**kwargs)
        cls.column_names = [c for c in cls.columns.keys()]
        cls.num_columns = len(cls.columns)

    def data_dict(self) -> Dict[str, List]:
        """Returns just the instance's data into a dict.

        Unlike load_into_dict, this does not include any
        metadata or preserve column naming quirks"""
        d = {k: getattr(self, k) for k in self.column_names}
        d["extras"] = self.extras
        return d

    def as_bsv_str(
        self,
        fill_columns: bool = True,
        trim_columns: bool = True,
        trim_values: bool = False,
    ) -> str:
        """
        The opposite function as load_into_row, more or less.

        For the params, the descriptions given below are all what they do if True
        :param fill_columns: add empty columns where the input had entirely missing columns
        :param trim_columns: totally drops the extra columns from output
        :param trim_values: truncates spare values within columns in the output
        :return: This TableRow dataclass except returned to its BSV row string format
        """
        o: List[str] = []
        for c_name, c_def in self.columns.items():
            v = getattr(self, c_name, None)
            if v is None:
                if not fill_columns:
                    break  # there's no more columns after the missing one
                v = ""
            o.append(c_def.to_str(v, trim_values))
        if not trim_columns and self.extras:
            o.append(RECORD.join(UNIT.join(s for s in z) for z in self.extras))
        return RECORD.join(o)

    @classmethod
    def load_into_row(cls, data: List[str], row_index: int = -1) -> RowType:
        """Reads data into an instance of cls.
        See the docstring for load_into_dict for details on the params."""
        trimmed_data_copy: List[str | None] = deepcopy(data)
        extras: List[str] = trimmed_data_copy[cls.num_columns :]
        trimmed_data_copy = trimmed_data_copy[: cls.num_columns] + (
            [None] * (cls.num_columns - len(trimmed_data_copy))
        )  # there has to be a cleaner way to pad out short rows
        return cls(*trimmed_data_copy, extras, row_index)

    @classmethod
    def load_into_dict(cls, data: List[str], row_index: int = -1) -> Dict:
        """
        Closer to csv.DictReader than load_into_data_row in spite sharing a function
        signature with the later.

        This function does zero validation of the input rows and puts everything
        (both column names and values) into strings or lists thereof.

        :param data: the list of strings to process
        :param row_index: for creating the metadata
        :return: A dict where each column name is a (string) key.
            Any non-string keys in this dict are either metadata, o[151374],
            or are for spillover values when there are more values than columns, o[None].

            - o[None] == None indicates that there are no spillover values in the row.
            - Likewise, o[column] == None indicates that the row ran out of values before reaching this column.
            - A column with an empty value has a value of '' in the dict.

            Each cell is turned into a list of objects that are split according to the
            column definition.
        """
        meta = {"table_name": cls.original_name, "row_index": row_index}
        o = {None: []}
        for column_type, value in zip_longest(
            cls.columns.values(), data, fillvalue=None
        ):
            if column_type is None:  # extra fields
                o[column_type].append(value.split(UNIT))
                continue

            # the usual route
            column_name = column_type.name
            o[column_name] = column_type(value)
        o[151374] = meta  # 151 = IVI (looks like m), 374 = ETA
        return o

    @classmethod
    def load_from_raw_strings(
        cls, data: List[str], into_dicts: bool = False, row_index: int = -1
    ) -> RowType | Dict:
        """Entry point for both loading functions,
        useful for programmatic switching between reading into dicts or RowType instances"""
        if into_dicts:
            return cls.load_into_dict(data, row_index)
        return cls.load_into_row(data, row_index)


def _repr4table_row(self):
    """This should be __repr__ for TableRow,
    but it doesn't work properly when defined within the class directly"""
    return f"{self.__name__}_row({self.data_dict()})"


def new_table_type(
    name: str,
    *columns: ColumnDefinition,
    allow_short: bool = False,
    allow_extras: bool = False,
    error_collector: Optional[ErrorList] = None,
    **ns,
) -> "Type[RowType]":
    """Creates a new Class to contain rows from a table

    :param error_collector: a place to collect errors
    :param name: The name of the table.  Gets changed to "Name_row" in the output
    :param columns: Any number of ColumnDefinitions in order of the table's columns
    :param allow_short: Consider short rows to be valid
    :param allow_extras: Consider rows with extra values to be valid
    :param ns: other attributes to be put into the class' namespace
    :return: A new Class
    """
    # prep work
    columns_as_fields: List[Tuple] = []
    original_name = name
    name = to_class_name(name)

    ns = {  # class attributes
        "__name__": name,
        **ns,
        "original_name": original_name,
        "allow_short": allow_short,
        "allow_extras": allow_extras,
        "columns": {},
        "__repr__": _repr4table_row,
    }
    # turn the columns into Dataclass fields
    for c in columns:
        if isinstance(c, ColumnTemplate):
            c: ColumnDefinition = c(c.name)
            add_error2el(
                ColumnError("Attempted use of template column"), error_collector
            )
        n = "col_" + to_class_name(c.name)
        columns_as_fields.append((n, dataclasses.InitVar))
        ns["columns"][n] = c

    # extras and row_index defined here to play nice with dataclass __init__ definition
    columns_as_fields.append(("extras", dataclasses.InitVar[list]))
    columns_as_fields.append(
        ("row_index", dataclasses.InitVar[int], dataclasses.field(default=-1))
    )

    # create the new class
    return dataclasses.make_dataclass(
        f"{name}_row", columns_as_fields, bases=(TableRow,), namespace=ns,
    )


RowType = TypeVar("RowType", bound=TableRow)


def new_table_from_raw_lines(
    table_definition: RawLine, column_heads: RawLine, el: Optional[ErrorList] = None
) -> "Type[RowType]":
    """Read a pair of RawLine objects to create a new RowType class

    :param el: optional collector for errors
    :param table_definition: RawLine with the basic table information
    :param column_heads: RawLine containing the column setup information
    :return: a new subclass of TableRow for this new table
    """
    # There clearly has to be a better way to split a string into
    # a list with meaningful order when not all components are present
    tad = {
        "original_name": "",
        "options": "",  # 1
        "comment": None,  # 2
        "client": None,  # 3
        "extras": [],  # 4
    }
    for attr, value in zip(tad.keys(), table_definition.content):
        if attr == "extras":
            break  # handle these a line later
        tad[attr] = value
    tad["extra_headers"] = table_definition.content[4:]
    tad["allow_short"] = "S" in tad["options"].upper()
    tad["allow_extras"] = "X" in tad["options"].upper()
    return new_table_type(
        tad["original_name"],
        *[column_from_BSV_string(c, el) for c in column_heads.content],
        **tad,
    )


def read_file_into_rows(
    f,
    errors: ErrorList = None,
    *,
    strictness: int = 23,
    into_dicts: bool = False,
    direct_iterator: Optional[Iterable[RawLine]] = None,
    acceptable_errors: List[Type[InputError]] = None,
    buffer: int = io.DEFAULT_BUFFER_SIZE,
):
    """Where all the magic occurs.  Reads the file into dataclasses or dicts.

    :param acceptable_errors: a list of types of acceptable error rather than a numeric value guess
    :param buffer: controls how much of the file to read at a times.
        No need to adjust this under normal circumstances.
    :param direct_iterator: alternate to f
    :param f: the BSV file-like object to read
    :param errors: writable
    :param strictness: stop processing and raise an exception at any error?
    :param into_dicts: Read the file into dicts?  If not, each table generates a
        subtype of dataclass in which the results are stored.
    :return: yields either a dict or a dataclass for each row of the input file
    """

    # setup
    last_ending: chr = ""
    rows: Iterator[RawLine] = iter(
        direct_iterator
    ) if direct_iterator else _split_file_into_rows(f, buffer)
    if errors is None:
        errors = ErrorList()
    if acceptable_errors is None:
        acceptable_errors = []
    current_table: Type[RowType] = type(
        RowType
    )  # this assignment is to make the linter shut up later
    tables: Dict[str, Type[RowType]] = {}
    if not strictness:
        strictness = 998

    # the actual processing loop
    for line in rows:
        if not line.ending and not line.content:
            continue  # file is all done

        # read table header
        # also covers the empty string at the start of reading
        if last_ending in FILE:
            # print("next table!")
            table_name = to_class_name(line.content[0])
            if table_name not in tables.keys():  # define a new table
                # next(rows) is called here and not in new_table() so
                # that last_ending may be properly set
                column_headers: RawLine = next(rows)

                tables[table_name] = new_table_from_raw_lines(line, column_headers)
                last_ending = column_headers.ending
            else:
                last_ending = line.ending
            current_table = tables[table_name]
            continue

        # read the row into an object (or collect an error)
        o = current_table.load_from_raw_strings(
            line.content, into_dicts, line.row_index
        )
        if not into_dicts:
            # handle errors
            errors.extend(o.errors)
            for e in o.errors:
                if e.severity > strictness and not any(
                    isinstance(e, a) for a in acceptable_errors
                ):
                    raise e
        yield o  # congratulations, we've parsed a new row!
        # save the ending for reading the next line
        last_ending = line.ending

    return errors


def bsv_dict2str(d: Dict[Any, List]) -> str:
    """The opposite of TableRow.load_into_dicts, sort of."""
    main_content = RECORD.join(
        (
            UNIT.join(str(i) for i in vals)
            for col, vals in d.items()
            if isinstance(col, str) and vals is not None
        )
    )
    if not d[None]:
        return main_content
    return RECORD.join([main_content] + [UNIT.join(i for i in c) for c in d[None]])


def write_from_dicts(f, rows: Iterable[Dict[Any, List]], *, use_n: bool = True):
    """Like the below write_to_file but takes an Iterable of Dicts instead of an Iterable
    of RowTypes."""
    first_row: bool = True
    for row in rows:
        if first_row:
            first_row = False
        else:
            f.write(GROUP + "\n" if use_n else "")
        f.write(bsv_dict2str(row))


def write_to_file(
    f,
    rows: Iterable[RowType],
    *,
    use_n: bool = True,
    sort: bool = False,
    fill_columns: bool = True,
    trim_columns: bool = False,
    trim_values: bool = False,
):
    """Writes the incoming RowTypes into a BSV file, f.

    Each time the specific Type[RowType] changes, a new table header is written (either
    the full header on first encounter or a quick reminder name change).

    If sort = True, each table's name should be written exactly once in the role of header
    """

    def wn(content):  # write with a newline ending, if applicable
        f.write(content + "\n" if use_n else "")

    def wx(content):  # write exactly what is passed
        f.write(content)

    if sort:
        rows = sorted(rows, key=_type_sort_key)
    table_list = set()
    current_table = None

    for row in rows:
        this_table = type(row)
        if this_table != current_table:
            if current_table is not None:
                wn(FILE)
            current_table = this_table
            if this_table not in table_list:
                wx(row.table_str(use_n))
                table_list.add(this_table)
            else:
                wx(this_table.original_name)
        wn(GROUP)
        wx(row.as_bsv_str(fill_columns, trim_columns, trim_values))


def _type_sort_key(t):
    return hash(type(t))
